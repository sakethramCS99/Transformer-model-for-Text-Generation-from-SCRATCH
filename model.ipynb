{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf74c47-9de9-4b98-9a0e-0a8669144dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f74dbb4-a8aa-40a4-905c-57a664652cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ef6d76dbf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "context_length = 16\n",
    "d_model = 128\n",
    "num_blocks = 8\n",
    "num_heads = 4\n",
    "learning_rate = 1e-4\n",
    "dropout = 0.05\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "eval_iters = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470844d2-6418-4cf8-b422-9d3dc235eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('textbook.txt'):\n",
    "    with open('textbook.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483fb569-d2fe-4382-afd0-155d5f6fc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = encoding.encode(text)\n",
    "max_token_value = max(tokenized_text) + 1\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=device)\n",
    "\n",
    "split_idx = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f158655f-0570-4006-8ef7-1780006502bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_model, out_features=self.d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.d_model * 4, out_features=self.d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e884f48-578b-4de6-9e7f-ba0e56eb1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.context_length = context_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.key_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.query_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones((self.context_length, self.context_length))))\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        assert T <= self.context_length\n",
    "        assert C == self.d_model\n",
    "        q = self.query_layer(x)\n",
    "        k = self.key_layer(x)\n",
    "        v = self.value_layer(x)\n",
    "\n",
    "        weights = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(input=weights, dim=-1)\n",
    "        weights = self.dropout_layer(weights)\n",
    "\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16675c2c-9215-42d9-bce8-9d55d0ba1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.heads = nn.ModuleList([Attention(head_size=self.head_size) for _ in range(self.num_heads)])\n",
    "        self.projection_layer = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection_layer(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f40436c-6263-4784-99a4-1ffa18e5a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.head_size = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.multi_head_attention_layer = MultiHeadAttention(head_size=self.head_size)\n",
    "        self.feed_forward_layer = FeedForward()\n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention_layer(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward_layer(self.layer_norm_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5f1ff6-109a-4d6a-9beb-cb9f137e36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout = dropout\n",
    "        self.max_token_value = max_token_value\n",
    "        self.token_embedding_lookup_table = nn.Embedding(num_embeddings=self.max_token_value + 1, embedding_dim=self.d_model)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*(\n",
    "                [TransformerBlock(num_heads=self.num_heads) for _ in range(self.num_blocks)] +\n",
    "                [nn.LayerNorm(self.d_model)]\n",
    "        ))\n",
    "        self.language_model_out_linear_layer = nn.Linear(in_features=self.d_model, out_features=self.max_token_value)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        position_encoding_lookup_table = torch.zeros(self.context_length, self.d_model)\n",
    "        position = torch.arange(0, self.context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))\n",
    "        position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_embedding = position_encoding_lookup_table[:T, :].to(device)\n",
    "        x = self.token_embedding_lookup_table(idx) + position_embedding\n",
    "        x = self.transformer_blocks(x)\n",
    "        logits = self.language_model_out_linear_layer(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_reshaped = logits.view(B * T, C)\n",
    "            targets_reshaped = targets.view(B * T)\n",
    "            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -self.context_length:]\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits_last_timestep = logits[:, -1, :]\n",
    "            probs = F.softmax(input=logits_last_timestep, dim=-1)\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb410486-0ecb-4193-adf5-479bcf63dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f70dbbd-8418-43d3-b351-288827c523e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[idx:idx + context_length] for idx in idxs]).to(device)\n",
    "    y = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x_batch, y_batch = get_batch(split)\n",
    "            logits, loss = model(x_batch, y_batch)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30de22f9-084a-4376-a212-5440aa276f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 11.652 Validation Loss: 11.607\n",
      "Step: 50 Training Loss: 10.497 Validation Loss: 10.619\n",
      "Step: 100 Training Loss: 9.542 Validation Loss: 9.962\n",
      "Step: 150 Training Loss: 8.546 Validation Loss: 9.359\n",
      "Step: 200 Training Loss: 7.512 Validation Loss: 8.751\n",
      "Step: 250 Training Loss: 6.747 Validation Loss: 8.194\n",
      "Step: 300 Training Loss: 6.11 Validation Loss: 8.077\n",
      "Step: 350 Training Loss: 5.663 Validation Loss: 7.837\n",
      "Step: 400 Training Loss: 5.26 Validation Loss: 7.786\n",
      "Step: 450 Training Loss: 4.858 Validation Loss: 7.766\n",
      "Step: 500 Training Loss: 4.507 Validation Loss: 7.781\n",
      "Step: 550 Training Loss: 4.203 Validation Loss: 7.763\n",
      "Step: 600 Training Loss: 3.898 Validation Loss: 7.959\n",
      "Step: 650 Training Loss: 3.621 Validation Loss: 7.911\n",
      "Step: 700 Training Loss: 3.297 Validation Loss: 7.803\n",
      "Step: 750 Training Loss: 3.006 Validation Loss: 7.605\n",
      "Step: 800 Training Loss: 2.777 Validation Loss: 7.827\n",
      "Step: 850 Training Loss: 2.544 Validation Loss: 7.744\n",
      "Step: 900 Training Loss: 2.354 Validation Loss: 7.824\n",
      "Step: 950 Training Loss: 2.179 Validation Loss: 7.96\n",
      "Step: 1000 Training Loss: 1.877 Validation Loss: 7.923\n",
      "Step: 1050 Training Loss: 1.729 Validation Loss: 7.815\n",
      "Step: 1100 Training Loss: 1.585 Validation Loss: 7.859\n",
      "Step: 1150 Training Loss: 1.44 Validation Loss: 7.822\n",
      "Step: 1200 Training Loss: 1.278 Validation Loss: 7.964\n",
      "Step: 1250 Training Loss: 1.189 Validation Loss: 8.046\n",
      "Step: 1300 Training Loss: 1.049 Validation Loss: 7.822\n",
      "Step: 1350 Training Loss: 0.937 Validation Loss: 8.23\n",
      "Step: 1400 Training Loss: 0.83 Validation Loss: 8.241\n",
      "Step: 1450 Training Loss: 0.779 Validation Loss: 8.214\n",
      "Step: 1500 Training Loss: 0.714 Validation Loss: 8.009\n",
      "Step: 1550 Training Loss: 0.652 Validation Loss: 8.094\n",
      "Step: 1600 Training Loss: 0.56 Validation Loss: 8.166\n",
      "Step: 1650 Training Loss: 0.521 Validation Loss: 8.212\n",
      "Step: 1700 Training Loss: 0.497 Validation Loss: 8.218\n",
      "Step: 1750 Training Loss: 0.438 Validation Loss: 8.081\n",
      "Step: 1800 Training Loss: 0.418 Validation Loss: 8.391\n",
      "Step: 1850 Training Loss: 0.361 Validation Loss: 8.421\n",
      "Step: 1900 Training Loss: 0.361 Validation Loss: 8.253\n",
      "Step: 1950 Training Loss: 0.336 Validation Loss: 8.464\n",
      "Step: 2000 Training Loss: 0.319 Validation Loss: 8.271\n",
      "Step: 2050 Training Loss: 0.298 Validation Loss: 8.899\n",
      "Step: 2100 Training Loss: 0.301 Validation Loss: 8.399\n",
      "Step: 2150 Training Loss: 0.275 Validation Loss: 8.647\n",
      "Step: 2200 Training Loss: 0.253 Validation Loss: 8.497\n",
      "Step: 2250 Training Loss: 0.248 Validation Loss: 8.767\n",
      "Step: 2300 Training Loss: 0.238 Validation Loss: 8.542\n",
      "Step: 2350 Training Loss: 0.228 Validation Loss: 8.732\n",
      "Step: 2400 Training Loss: 0.234 Validation Loss: 8.769\n",
      "Step: 2450 Training Loss: 0.219 Validation Loss: 8.511\n",
      "Step: 2500 Training Loss: 0.211 Validation Loss: 8.657\n",
      "Step: 2550 Training Loss: 0.208 Validation Loss: 9.024\n",
      "Step: 2600 Training Loss: 0.207 Validation Loss: 8.738\n",
      "Step: 2650 Training Loss: 0.201 Validation Loss: 9.016\n",
      "Step: 2700 Training Loss: 0.189 Validation Loss: 8.4\n",
      "Step: 2750 Training Loss: 0.182 Validation Loss: 8.728\n",
      "Step: 2800 Training Loss: 0.189 Validation Loss: 8.961\n",
      "Step: 2850 Training Loss: 0.182 Validation Loss: 8.883\n",
      "Step: 2900 Training Loss: 0.177 Validation Loss: 8.949\n",
      "Step: 2950 Training Loss: 0.153 Validation Loss: 8.796\n",
      "Step: 3000 Training Loss: 0.164 Validation Loss: 8.925\n",
      "Step: 3050 Training Loss: 0.175 Validation Loss: 8.934\n",
      "Step: 3100 Training Loss: 0.173 Validation Loss: 9.098\n",
      "Step: 3150 Training Loss: 0.164 Validation Loss: 8.953\n",
      "Step: 3200 Training Loss: 0.16 Validation Loss: 8.977\n",
      "Step: 3250 Training Loss: 0.151 Validation Loss: 9.165\n",
      "Step: 3300 Training Loss: 0.162 Validation Loss: 8.756\n",
      "Step: 3350 Training Loss: 0.164 Validation Loss: 9.167\n",
      "Step: 3400 Training Loss: 0.151 Validation Loss: 9.234\n",
      "Step: 3450 Training Loss: 0.142 Validation Loss: 8.844\n",
      "Step: 3500 Training Loss: 0.152 Validation Loss: 9.1\n",
      "Step: 3550 Training Loss: 0.137 Validation Loss: 9.235\n",
      "Step: 3600 Training Loss: 0.146 Validation Loss: 9.165\n",
      "Step: 3650 Training Loss: 0.145 Validation Loss: 8.98\n",
      "Step: 3700 Training Loss: 0.14 Validation Loss: 9.112\n",
      "Step: 3750 Training Loss: 0.129 Validation Loss: 9.235\n",
      "Step: 3800 Training Loss: 0.139 Validation Loss: 9.126\n",
      "Step: 3850 Training Loss: 0.147 Validation Loss: 9.107\n",
      "Step: 3900 Training Loss: 0.137 Validation Loss: 9.368\n",
      "Step: 3950 Training Loss: 0.138 Validation Loss: 9.06\n",
      "Step: 4000 Training Loss: 0.13 Validation Loss: 9.135\n",
      "Step: 4050 Training Loss: 0.125 Validation Loss: 9.105\n",
      "Step: 4100 Training Loss: 0.138 Validation Loss: 9.182\n",
      "Step: 4150 Training Loss: 0.132 Validation Loss: 9.275\n",
      "Step: 4200 Training Loss: 0.126 Validation Loss: 9.355\n",
      "Step: 4250 Training Loss: 0.119 Validation Loss: 9.484\n",
      "Step: 4300 Training Loss: 0.133 Validation Loss: 9.187\n",
      "Step: 4350 Training Loss: 0.132 Validation Loss: 9.451\n",
      "Step: 4400 Training Loss: 0.128 Validation Loss: 9.456\n",
      "Step: 4450 Training Loss: 0.121 Validation Loss: 9.356\n",
      "Step: 4500 Training Loss: 0.136 Validation Loss: 9.397\n",
      "Step: 4550 Training Loss: 0.125 Validation Loss: 9.584\n",
      "Step: 4600 Training Loss: 0.113 Validation Loss: 9.271\n",
      "Step: 4650 Training Loss: 0.121 Validation Loss: 9.294\n",
      "Step: 4700 Training Loss: 0.124 Validation Loss: 9.249\n",
      "Step: 4750 Training Loss: 0.124 Validation Loss: 9.21\n",
      "Step: 4800 Training Loss: 0.121 Validation Loss: 9.234\n",
      "Step: 4850 Training Loss: 0.133 Validation Loss: 9.385\n",
      "Step: 4900 Training Loss: 0.122 Validation Loss: 9.757\n",
      "Step: 4950 Training Loss: 0.107 Validation Loss: 9.814\n",
      "Step: 4999 Training Loss: 0.123 Validation Loss: 9.408\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "tracked_losses = list()\n",
    "for step in range(max_iters):\n",
    "    if step % eval_iters == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        tracked_losses.append(losses)\n",
    "        print('Step:', step, 'Training Loss:', round(losses['train'].item(), 3), 'Validation Loss:', round(losses['valid'].item(), 3))\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae014f9a-c50d-46d1-9b8a-35085fc68412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your starting prompt:  developers use AI to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "developers use AI to align them with the end user’s needs. Developers must demonstrate how AI can solve specific pain points, whether through improving efficiency, reducing costs, or enhancing user satisfaction. Case studies and success stories perform their tasks reliably, ethically, and transparently.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "model.eval()\n",
    "start = input(\"enter your starting prompt: \")\n",
    "start_ids = encoding.encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "y = model.generate(x, max_new_tokens=50)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(encoding.decode(y[0].tolist()))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88a617-68e4-4913-a230-10bb75c2ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfdb1a-52c8-421f-a483-997f6e989d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17380c9-7541-47bd-916a-afe597236d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f095f83-1484-417c-a366-99f7b69eab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e50f9-519e-46f0-ac47-f7f95a0ea6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "567"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
